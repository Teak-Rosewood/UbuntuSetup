{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "batch_size = 64 \n",
    "epochs = 100  \n",
    "latent_dim = 256\n",
    "num_samples = 10000\n",
    "data_path = \"data/hin.txt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 2979\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 90\n",
      "Max sequence length for inputs: 107\n",
      "Max sequence length for outputs: 123\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing\n",
    "\n",
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "for line in lines[: min(num_samples, len(lines) - 1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    # We use \"tab\" as the \"start sequence\" character\n",
    "    # for the targets, and \"\\n\" as \"end sequence\" character.\n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)\n",
    "\n",
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))\n",
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)\n",
    "max_encoder_seq_length = max([len(txt) for txt in input_texts])\n",
    "max_decoder_seq_length = max([len(txt) for txt in target_texts])\n",
    "\n",
    "print(\"Number of samples:\", len(input_texts))\n",
    "print(\"Number of unique input tokens:\", num_encoder_tokens)\n",
    "print(\"Number of unique output tokens:\", num_decoder_tokens)\n",
    "print(\"Max sequence length for inputs:\", max_encoder_seq_length)\n",
    "print(\"Max sequence length for outputs:\", max_decoder_seq_length)\n",
    "\n",
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])\n",
    "\n",
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "('Keyword argument not understood:', 'return_state')",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/home/blank/projects/labs/dl_lab/week_8/week_8.ipynb Cell 4\u001b[0m line \u001b[0;36m1\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blank/projects/labs/dl_lab/week_8/week_8.ipynb#W3sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m \u001b[39m# GRU Model\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blank/projects/labs/dl_lab/week_8/week_8.ipynb#W3sZmlsZQ%3D%3D?line=17'>18</a>\u001b[0m encoder_inputs \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39mInput(shape\u001b[39m=\u001b[39m(\u001b[39mNone\u001b[39;00m, num_encoder_tokens))\n\u001b[0;32m---> <a href='vscode-notebook-cell:/home/blank/projects/labs/dl_lab/week_8/week_8.ipynb#W3sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m encoder \u001b[39m=\u001b[39m keras\u001b[39m.\u001b[39;49mlayers\u001b[39m.\u001b[39;49mGRUCell(latent_dim, return_state\u001b[39m=\u001b[39;49m\u001b[39mTrue\u001b[39;49;00m)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blank/projects/labs/dl_lab/week_8/week_8.ipynb#W3sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m encoder_outputs, state_h, state_c \u001b[39m=\u001b[39m encoder(encoder_inputs)\n\u001b[1;32m     <a href='vscode-notebook-cell:/home/blank/projects/labs/dl_lab/week_8/week_8.ipynb#W3sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m encoder_states \u001b[39m=\u001b[39m [state_h, state_c]\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/rnn/gru.py:152\u001b[0m, in \u001b[0;36mGRUCell.__init__\u001b[0;34m(self, units, activation, recurrent_activation, use_bias, kernel_initializer, recurrent_initializer, bias_initializer, kernel_regularizer, recurrent_regularizer, bias_regularizer, kernel_constraint, recurrent_constraint, bias_constraint, dropout, recurrent_dropout, reset_after, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    149\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_enable_caching_device \u001b[39m=\u001b[39m kwargs\u001b[39m.\u001b[39mpop(\n\u001b[1;32m    150\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39menable_caching_device\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39mFalse\u001b[39;00m\n\u001b[1;32m    151\u001b[0m     )\n\u001b[0;32m--> 152\u001b[0m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    153\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39munits \u001b[39m=\u001b[39m units\n\u001b[1;32m    154\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mactivation \u001b[39m=\u001b[39m activations\u001b[39m.\u001b[39mget(activation)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/layers/rnn/dropout_rnn_cell_mixin.py:43\u001b[0m, in \u001b[0;36mDropoutRNNCellMixin.__init__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[1;32m     42\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_create_non_trackable_mask_cache()\n\u001b[0;32m---> 43\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py:3820\u001b[0m, in \u001b[0;36mBaseRandomLayer.__init__\u001b[0;34m(self, seed, force_generator, rng_type, **kwargs)\u001b[0m\n\u001b[1;32m   3792\u001b[0m \u001b[39m@tf\u001b[39m\u001b[39m.\u001b[39m__internal__\u001b[39m.\u001b[39mtracking\u001b[39m.\u001b[39mno_automatic_dependency_tracking\n\u001b[1;32m   3793\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m__init__\u001b[39m(\n\u001b[1;32m   3794\u001b[0m     \u001b[39mself\u001b[39m, seed\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, force_generator\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m, rng_type\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs\n\u001b[1;32m   3795\u001b[0m ):\n\u001b[1;32m   3796\u001b[0m     \u001b[39m\"\"\"Initialize the BaseRandomLayer.\u001b[39;00m\n\u001b[1;32m   3797\u001b[0m \n\u001b[1;32m   3798\u001b[0m \u001b[39m    Note that the constructor is annotated with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   3818\u001b[0m \u001b[39m        *class\u001b[39;00m\n\u001b[1;32m   3819\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 3820\u001b[0m     \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49m\u001b[39m__init__\u001b[39;49m(\u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m   3821\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_random_generator \u001b[39m=\u001b[39m backend\u001b[39m.\u001b[39mRandomGenerator(\n\u001b[1;32m   3822\u001b[0m         seed, force_generator\u001b[39m=\u001b[39mforce_generator, rng_type\u001b[39m=\u001b[39mrng_type\n\u001b[1;32m   3823\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/tensorflow/python/trackable/base.py:204\u001b[0m, in \u001b[0;36mno_automatic_dependency_tracking.<locals>._method_wrapper\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    202\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m \u001b[39mFalse\u001b[39;00m  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 204\u001b[0m   result \u001b[39m=\u001b[39m method(\u001b[39mself\u001b[39;49m, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    205\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_self_setattr_tracking \u001b[39m=\u001b[39m previous_value  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/engine/base_layer.py:340\u001b[0m, in \u001b[0;36mLayer.__init__\u001b[0;34m(self, trainable, name, dtype, dynamic, **kwargs)\u001b[0m\n\u001b[1;32m    329\u001b[0m allowed_kwargs \u001b[39m=\u001b[39m {\n\u001b[1;32m    330\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_dim\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    331\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39minput_shape\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mimplementation\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    338\u001b[0m }\n\u001b[1;32m    339\u001b[0m \u001b[39m# Validate optional keyword arguments.\u001b[39;00m\n\u001b[0;32m--> 340\u001b[0m generic_utils\u001b[39m.\u001b[39;49mvalidate_kwargs(kwargs, allowed_kwargs)\n\u001b[1;32m    342\u001b[0m \u001b[39m# Mutable properties\u001b[39;00m\n\u001b[1;32m    343\u001b[0m \u001b[39m# Indicates whether the layer's weights are updated during training\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[39m# and whether the layer's updates are run during training.\u001b[39;00m\n\u001b[1;32m    345\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\n\u001b[1;32m    346\u001b[0m     \u001b[39misinstance\u001b[39m(trainable, \u001b[39mbool\u001b[39m)\n\u001b[1;32m    347\u001b[0m     \u001b[39mor\u001b[39;00m (\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    350\u001b[0m     )\n\u001b[1;32m    351\u001b[0m ):\n",
      "File \u001b[0;32m~/miniconda3/envs/tf/lib/python3.9/site-packages/keras/src/utils/generic_utils.py:514\u001b[0m, in \u001b[0;36mvalidate_kwargs\u001b[0;34m(kwargs, allowed_kwargs, error_message)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[39mfor\u001b[39;00m kwarg \u001b[39min\u001b[39;00m kwargs:\n\u001b[1;32m    513\u001b[0m     \u001b[39mif\u001b[39;00m kwarg \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m allowed_kwargs:\n\u001b[0;32m--> 514\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(error_message, kwarg)\n",
      "\u001b[0;31mTypeError\u001b[0m: ('Keyword argument not understood:', 'return_state')"
     ]
    }
   ],
   "source": [
    "# LSTM Model\n",
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = keras.layers.LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "lstm = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# GRU Model\n",
    "# encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "# encoder = keras.layers.GRUCell(latent_dim, return_state=True)\n",
    "# encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "\n",
    "# encoder_states = [state_h, state_c]\n",
    "\n",
    "# decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "# decoder_lstm = keras.layers.GRUCell(latent_dim, return_sequences=True, return_state=True)\n",
    "# decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "# decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "# decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# gru = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
